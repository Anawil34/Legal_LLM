{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9ac68f-bb5e-49e0-ae32-8d9814fd8308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction'],\n",
       "    num_rows: 5835\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "test_ds = load_from_disk('./data_hf/test/')\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e977f584-11a8-48b3-8408-d85182a5a44f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-08 15:00:56,219 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-08 15:00:56,220 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-08 15:00:56,221 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-08 15:00:56,222 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-05-08 15:00:56,637 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/08/2024 15:00:56 - INFO - llmtuner.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:726] 2024-05-08 15:00:56,864 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-08 15:00:56,869 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/08/2024 15:00:56 - INFO - llmtuner.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "05/08/2024 15:00:56 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "05/08/2024 15:00:56 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-08 15:00:57,921 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-08 15:00:57,926 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:726] 2024-05-08 15:00:58,174 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-08 15:00:58,179 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:726] 2024-05-08 15:00:58,417 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-08 15:00:58,423 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[WARNING|quantization_config.py:282] 2024-05-08 15:00:58,468 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-08 15:00:58,472 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/model.safetensors\n",
      "[INFO|modeling_utils.py:1494] 2024-05-08 15:00:58,523 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-08 15:00:58,530 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4170] 2024-05-08 15:01:02,952 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-08 15:01:02,954 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-08 15:01:03,197 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/89e4fd4e68bf61861110149fa59990e3bbcab6eb/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-08 15:01:03,200 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,497 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,498 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,499 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,500 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-05-08 15:01:03,905 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,908 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,908 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,909 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2085] 2024-05-08 15:01:03,909 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-05-08 15:01:04,302 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[WARNING|logging.py:329] 2024-05-08 15:01:10,998 >> Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/08/2024 15:01:11 - INFO - llmtuner.model.adapter - Loaded adapter(s): llama3_lora\n",
      "05/08/2024 15:01:11 - INFO - llmtuner.model.loader - all params: 8051232768\n"
     ]
    }
   ],
   "source": [
    "from llmtuner.chat import ChatModel\n",
    "from llmtuner.extras.misc import torch_gc\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\" ../../llm_model/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=\"saves/llama3-8b/lora/sft/\",            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    "  use_unsloth=True,                     # use UnslothAI's LoRA optimization for 2x faster generation\n",
    ")\n",
    "chat_model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ae316-a181-4f6a-b258-fb0e8c253d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample):\n",
    "    query = sample[\"prompt\"]\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    new_text = chat_model.chat(messages)\n",
    "    return {\"predict\": new_text[0].response_text}\n",
    "\n",
    "test_ds = test_ds.map(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32cbbe7-0770-4f6c-a3d3-3d54e7e93945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ในฐานะทนายความ ที่ทำหน้าที่ตรวจสอบเอกสารนิติบุคคลเกี่ยวกับอำนาจกรรมการในการลงนามนิติกรรมสัญญา หน้าที่ของคุณคือการตอบคำถามที่ให้จากข้อมูลต่อไปนี้:\\n\\nเงื่อนไข: ลายมือชื่อของกรรมการคนใดคนหนึ่งลงลายมือชื่อ เป็นสำคัญผูกพันบริษัทได้\\nรายชื่อกรรมการ: นาย ก., นาย ข., นาย ค., นาย ง., นาย จ., นาย ฉ.\\nคำถาม: นาย ก. สามารถ การขอวีซ่า ได้หรือไม่ ?\\n\\nทำตามขั้นตอนเหล่านี้เพื่อตัดสินใจ:\\n\\nขั้นตอนที่ 1: วิเคราะห์เงื่อนไขที่กำหนดไว้ ลายมือชื่อของกรรมการคนใดคนหนึ่งลงลายมือชื่อ เป็นสำคัญผูกพันบริษัทได้\\n- พิจารณาเงื่อนไขหรือบริบทที่ให้มาอย่างละเอียด\\n- พิจารณาข้อยกเว้นต่างๆที่อยู่ในเงื่อนไข\\n- ระบุข้อกำหนดหรือเงื่อนไขสำคัญที่เกี่ยวข้องกับการดำเนินการทางกฎหมายที่กำหนด\\n\\nขั้นตอนที่ 2: ตรวจสอบรายชื่อกรรมการ\\n- ค้นหาชื่อ นาย ก. ในรายชื่อกรรมการที่ให้มานาย ก., นาย ข., นาย ค., นาย ง., นาย จ., นาย ฉ. อยู่ในรายชื่อของกรรมการบริษัทนั้นหรือไม่\\n\\nขั้นตอนที่ 3: พิจารณาการดำเนินการทางกฎหมายที่เฉพาะเจาะจง\\n- วิเคราะห์ความหมายและข้อกำหนดของ การขอวีซ่า\\n- หากเงื่อนไขไม่ได้เฉพาะเจาะจงถึงชื่อคน พิจารณาเฉพาะเงื่อนไขและจำนวนการลงนามของกรรมการ\\n- พิจารณาว่า การขอวีซ่า เกี่ยวข้องหรือได้รับอนุญาตภายใต้เงื่อนไขที่กำหนดไว้หรือไม่\\n\\nขั้นตอนที่ 4: ประเมินความสามารถของ นาย ก. ในการดำเนินการ การขอวีซ่า\\n- พิจารณาเงื่อนไขที่กำหนด บทบาทของ นาย ก. และลักษณะของ การขอวีซ่า\\n- ตัดสินว่า นาย ก. มีสิทธิ์ในการดำเนินการ การขอวีซ่า ตามเงื่อนไขที่กำหนดไว้หรือไม่\\n\\nขั้นตอนที่ 5: ให้คำตอบสรุป\\n- สรุปการวิเคราะห์และการประเมินของคุณ\\n- ตอบคำถาม \"นาย ก. สามารถ การขอวีซ่า ได้หรือไม่?\" ด้วย \"ใช่\" หรือ \"ไม่\" ตามผลการประเมิน\\n\\nรูปแบบเอาต์พุต:\\nคำตอบสุดท้าย: [ใช่/ไม่]\\n',\n",
       " 'predict': 'ใช่'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2666a7e-0dc9-4a15-9b3f-f795837a87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_predict(sample):\n",
    "    assert sample[\"predict\"] in [\"ใช่\", \"ไม่\"], \"error\" \n",
    "    \n",
    "    if sample[\"predict\"] == \"ใช่\":\n",
    "        return {\"pred_map\": 1}\n",
    "    elif sample[\"predict\"] == \"ไม่\":\n",
    "        return {\"pred_map\": 0}\n",
    "\n",
    "test_ds = test_ds.map(map_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0da3f961-7ed0-4ab3-90e2-d2476ecce931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ในฐานะทนายความ ที่ทำหน้าที่ตรวจสอบเอกสารนิติบุคคลเกี่ยวกับอำนาจกรรมการในการลงนามนิติกรรมสัญญา หน้าที่ของคุณคือการตอบคำถามที่ให้จากข้อมูลต่อไปนี้:\\n\\nเงื่อนไข: ลายมือชื่อของกรรมการคนใดคนหนึ่งลงลายมือชื่อ เป็นสำคัญผูกพันบริษัทได้\\nรายชื่อกรรมการ: นาย ก., นาย ข., นาย ค., นาย ง., นาย จ., นาย ฉ.\\nคำถาม: นาย ก. สามารถ การขอวีซ่า ได้หรือไม่ ?\\n\\nทำตามขั้นตอนเหล่านี้เพื่อตัดสินใจ:\\n\\nขั้นตอนที่ 1: วิเคราะห์เงื่อนไขที่กำหนดไว้ ลายมือชื่อของกรรมการคนใดคนหนึ่งลงลายมือชื่อ เป็นสำคัญผูกพันบริษัทได้\\n- พิจารณาเงื่อนไขหรือบริบทที่ให้มาอย่างละเอียด\\n- พิจารณาข้อยกเว้นต่างๆที่อยู่ในเงื่อนไข\\n- ระบุข้อกำหนดหรือเงื่อนไขสำคัญที่เกี่ยวข้องกับการดำเนินการทางกฎหมายที่กำหนด\\n\\nขั้นตอนที่ 2: ตรวจสอบรายชื่อกรรมการ\\n- ค้นหาชื่อ นาย ก. ในรายชื่อกรรมการที่ให้มานาย ก., นาย ข., นาย ค., นาย ง., นาย จ., นาย ฉ. อยู่ในรายชื่อของกรรมการบริษัทนั้นหรือไม่\\n\\nขั้นตอนที่ 3: พิจารณาการดำเนินการทางกฎหมายที่เฉพาะเจาะจง\\n- วิเคราะห์ความหมายและข้อกำหนดของ การขอวีซ่า\\n- หากเงื่อนไขไม่ได้เฉพาะเจาะจงถึงชื่อคน พิจารณาเฉพาะเงื่อนไขและจำนวนการลงนามของกรรมการ\\n- พิจารณาว่า การขอวีซ่า เกี่ยวข้องหรือได้รับอนุญาตภายใต้เงื่อนไขที่กำหนดไว้หรือไม่\\n\\nขั้นตอนที่ 4: ประเมินความสามารถของ นาย ก. ในการดำเนินการ การขอวีซ่า\\n- พิจารณาเงื่อนไขที่กำหนด บทบาทของ นาย ก. และลักษณะของ การขอวีซ่า\\n- ตัดสินว่า นาย ก. มีสิทธิ์ในการดำเนินการ การขอวีซ่า ตามเงื่อนไขที่กำหนดไว้หรือไม่\\n\\nขั้นตอนที่ 5: ให้คำตอบสรุป\\n- สรุปการวิเคราะห์และการประเมินของคุณ\\n- ตอบคำถาม \"นาย ก. สามารถ การขอวีซ่า ได้หรือไม่?\" ด้วย \"ใช่\" หรือ \"ไม่\" ตามผลการประเมิน\\n\\nรูปแบบเอาต์พุต:\\nคำตอบสุดท้าย: [ใช่/ไม่]\\n',\n",
       " 'predict': 'ใช่',\n",
       " 'pred_map': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146c4717-f4ff-432a-936f-e03ef1633bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>5830</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>5831</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>5832</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>5833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>5834</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5835 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  answer\n",
       "0        0     1.0\n",
       "1        1     1.0\n",
       "2        2     1.0\n",
       "3        3     NaN\n",
       "4        4     NaN\n",
       "...    ...     ...\n",
       "5830  5830     NaN\n",
       "5831  5831     NaN\n",
       "5832  5832     NaN\n",
       "5833  5833     NaN\n",
       "5834  5834     NaN\n",
       "\n",
       "[5835 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ee54813-2d52-432f-be83-9745eb10e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[3:, \"answer\"] = test_ds[\"pred_map\"][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ced69c1-2504-4f34-b11b-11a993b692ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>5830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>5831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>5832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>5833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>5834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5835 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  answer\n",
       "0        0       1\n",
       "1        1       1\n",
       "2        2       1\n",
       "3        3       1\n",
       "4        4       1\n",
       "...    ...     ...\n",
       "5830  5830       0\n",
       "5831  5831       1\n",
       "5832  5832       1\n",
       "5833  5833       1\n",
       "5834  5834       0\n",
       "\n",
       "[5835 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[\"answer\"] = submission[\"answer\"].astype(int)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c510587-95d0-4c50-8670-ead341732626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "1    3482\n",
       "0    2353\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[\"answer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74c8ba12-68c2-4f80-8b32-0ff90352b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
